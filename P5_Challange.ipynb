{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#import all the required python packages\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2\n",
    "import glob\n",
    "import time\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from skimage.feature import hog\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from scipy.ndimage.measurements import label\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a function to return HOG features and visualization\n",
    "def get_hog_features(img, orient, pix_per_cell, cell_per_block, \n",
    "                        vis=False, feature_vec=True):\n",
    "    # Call with two outputs if vis==True\n",
    "    if vis == True:\n",
    "        features, hog_image = hog(img, orientations=orient, \n",
    "                                  pixels_per_cell=(pix_per_cell, pix_per_cell),\n",
    "                                  cells_per_block=(cell_per_block, cell_per_block), \n",
    "                                  transform_sqrt=False, \n",
    "                                  visualise=vis, feature_vector=feature_vec)\n",
    "        return features, hog_image\n",
    "    # Otherwise call with one output\n",
    "    else:      \n",
    "        features = hog(img, orientations=orient, \n",
    "                       pixels_per_cell=(pix_per_cell, pix_per_cell),\n",
    "                       cells_per_block=(cell_per_block, cell_per_block), \n",
    "                       transform_sqrt=False, \n",
    "                       visualise=vis, feature_vector=feature_vec)\n",
    "        return features\n",
    "\n",
    "def bin_spatial(img, size=(32, 32)):\n",
    "    color1 = cv2.resize(img[:,:,0], size).ravel()\n",
    "    color2 = cv2.resize(img[:,:,1], size).ravel()\n",
    "    color3 = cv2.resize(img[:,:,2], size).ravel()\n",
    "    return np.hstack((color1, color2, color3))\n",
    "                        \n",
    "def color_hist(img, nbins=32):    #bins_range=(0, 256)\n",
    "    # Compute the histogram of the color channels separately\n",
    "    channel1_hist = np.histogram(img[:,:,0], bins=nbins)\n",
    "    channel2_hist = np.histogram(img[:,:,1], bins=nbins)\n",
    "    channel3_hist = np.histogram(img[:,:,2], bins=nbins)\n",
    "    # Concatenate the histograms into a single feature vector\n",
    "    hist_features = np.concatenate((channel1_hist[0], channel2_hist[0], channel3_hist[0]))\n",
    "    # Return the individual histograms, bin_centers and feature vector\n",
    "    return hist_features\n",
    "\n",
    "# Define a function to extract features from a list of images\n",
    "# Have this function call bin_spatial() and color_hist()\n",
    "def extract_features(imgs, color_space='RGB', spatial_size=(32, 32),\n",
    "                        hist_bins=32, orient=9, \n",
    "                        pix_per_cell=8, cell_per_block=2, hog_channel=0,\n",
    "                        spatial_feat=True, hist_feat=True, hog_feat=True):\n",
    "    # Create a list to append feature vectors to\n",
    "    features = []\n",
    "    # Iterate through the list of images\n",
    "    for file in imgs:\n",
    "        file_features = []\n",
    "        # Read in each one by one\n",
    "        image = cv2.imread(file)\n",
    "        #image = cv2.cvtColor(image,cv2.COLOR_BGR2RGB)\n",
    "        # apply color conversion if other than 'RGB'\n",
    "        if color_space != 'RGB':\n",
    "            if color_space == 'HSV':\n",
    "                feature_image = cv2.cvtColor(image, cv2.COLOR_RGB2HSV)\n",
    "            elif color_space == 'LUV':\n",
    "                feature_image = cv2.cvtColor(image, cv2.COLOR_RGB2LUV)\n",
    "            elif color_space == 'HLS':\n",
    "                feature_image = cv2.cvtColor(image, cv2.COLOR_RGB2HLS)\n",
    "            elif color_space == 'YUV':\n",
    "                feature_image = cv2.cvtColor(image, cv2.COLOR_RGB2YUV)\n",
    "            elif color_space == 'YCrCb':\n",
    "                feature_image = cv2.cvtColor(image, cv2.COLOR_RGB2YCrCb)\n",
    "        else: feature_image = np.copy(image)      \n",
    "\n",
    "        if spatial_feat == True:\n",
    "            spatial_features = bin_spatial(feature_image, size=spatial_size)\n",
    "            file_features.append(spatial_features)\n",
    "        if hist_feat == True:\n",
    "            # Apply color_hist()\n",
    "            hist_features = color_hist(feature_image, nbins=hist_bins)\n",
    "            file_features.append(hist_features)\n",
    "        if hog_feat == True:\n",
    "        # Call get_hog_features() with vis=False, feature_vec=True\n",
    "            if hog_channel == 'ALL':\n",
    "                hog_features = []\n",
    "                for channel in range(feature_image.shape[2]):\n",
    "                    hog_features.append(get_hog_features(feature_image[:,:,channel], \n",
    "                                        orient, pix_per_cell, cell_per_block, \n",
    "                                        vis=False, feature_vec=True))\n",
    "                hog_features = np.ravel(hog_features)        \n",
    "            else:\n",
    "                hog_features = get_hog_features(feature_image[:,:,hog_channel], orient, \n",
    "                            pix_per_cell, cell_per_block, vis=False, feature_vec=True)\n",
    "            # Append the new feature vector to the features list\n",
    "            file_features.append(hog_features)\n",
    "        features.append(np.concatenate(file_features))\n",
    "    # Return list of feature vectors\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9050\n",
      "8871\n"
     ]
    }
   ],
   "source": [
    "# Read in car and non-car images\n",
    "cars = []\n",
    "notcars = []\n",
    "\n",
    "notcars = glob.glob('./data/non-vehicles/Extras/*.PNG') + glob.glob('./data/non-vehicles/GTI/*.PNG') \\\n",
    "            + glob.glob(\"./data/non-vehicles/Project_Video/*.jpg\") \n",
    "\n",
    "cars = glob.glob('./data/vehicles/GTI_Far/*.PNG') + glob.glob('./data/vehicles/GTI_Left/*.PNG')+ \\\n",
    "        glob.glob('./data/vehicles/GTI_MiddleClose/*.PNG')+ glob.glob('./data/vehicles/GTI_Right/*.PNG') + \\\n",
    "         glob.glob('./data/vehicles/KITTI_extracted/*.PNG')+ glob.glob(\"./data/vehicles/Project_Video/*.jpg\") \n",
    "            \n",
    "print(len(notcars))\n",
    "print(len(cars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Conda\\envs\\my_conda\\lib\\site-packages\\skimage\\feature\\_hog.py:119: skimage_deprecation: Default value of `block_norm`==`L1` is deprecated and will be changed to `L2-Hys` in v0.15\n",
      "  'be changed to `L2-Hys` in v0.15', skimage_deprecation)\n"
     ]
    }
   ],
   "source": [
    "color_space = 'YCrCb' # Can be RGB, HSV, LUV, HLS, YUV, YCrCb\n",
    "orient = 16  # HOG orientations\n",
    "pix_per_cell = 8 # HOG pixels per cell\n",
    "cell_per_block = 8 # HOG cells per block\n",
    "hog_channel = \"ALL\" # Can be 0, 1, 2, or \"ALL\"\n",
    "spatial_size = (32, 32) # Spatial binning dimensions\n",
    "hist_bins = 32    # Number of histogram bins\n",
    "spatial_feat = True # Spatial features on or off\n",
    "hist_feat = True # Histogram features on or off\n",
    "hog_feat = True # HOG features on or off\n",
    "\n",
    "\n",
    "car_features = extract_features(cars, color_space=color_space, \n",
    "                        spatial_size=spatial_size, hist_bins=hist_bins, \n",
    "                        orient=orient, pix_per_cell=pix_per_cell, \n",
    "                        cell_per_block=cell_per_block, \n",
    "                        hog_channel=hog_channel, spatial_feat=spatial_feat, \n",
    "                        hist_feat=hist_feat, hog_feat=hog_feat)\n",
    "notcar_features = extract_features(notcars, color_space=color_space, \n",
    "                        spatial_size=spatial_size, hist_bins=hist_bins, \n",
    "                        orient=orient, pix_per_cell=pix_per_cell, \n",
    "                        cell_per_block=cell_per_block, \n",
    "                        hog_channel=hog_channel, spatial_feat=spatial_feat, \n",
    "                        hist_feat=hist_feat, hog_feat=hog_feat)\n",
    "\n",
    "X = np.vstack((car_features, notcar_features,car_features)).astype(np.float64)                        \n",
    "# Fit a per-column scaler\n",
    "X_scaler = StandardScaler().fit(X)\n",
    "# Apply the scaler to X\n",
    "scaled_X = X_scaler.transform(X)\n",
    "\n",
    "# Define the labels vector\n",
    "y = np.hstack((np.ones(len(car_features)), np.zeros(len(notcar_features))\n",
    "               , np.ones(len(car_features))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using: 16 orientations 8 pixels per cell and 8 cells per block\n",
      "Feature vector length: 6240\n",
      "106.26 Seconds to train SVC...\n",
      "Best score for data1: 0.995007698409\n",
      "Best C: 1\n",
      "Test Accuracy of SVC =  0.9959\n"
     ]
    }
   ],
   "source": [
    "# Split up data into randomized training and test sets\n",
    "rand_state = np.random.randint(0, 100)\n",
    "parameters = {'C':[1, 10]}\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    scaled_X, y, test_size=0.2, random_state=rand_state)\n",
    "\n",
    "print('Using:',orient,'orientations',pix_per_cell,\n",
    "    'pixels per cell and', cell_per_block,'cells per block')\n",
    "print('Feature vector length:', len(X_train[0]))\n",
    "# Use a linear SVC \n",
    "svc = LinearSVC()\n",
    "clf = GridSearchCV(svc, parameters)\n",
    "# Check the training time for the SVC\n",
    "t=time.time()\n",
    "clf.fit(X_train, y_train)\n",
    "t2 = time.time()\n",
    "print(round(t2-t, 2), 'Seconds to train SVC...')\n",
    "# View the accuracy score\n",
    "print('Best score for data1:', clf.best_score_) \n",
    "# View the best parameters for the model found using grid search\n",
    "print('Best C:',clf.best_estimator_.C) \n",
    "# Check the score of the SVC\n",
    "print('Test Accuracy of SVC = ', round(clf.score(X_test, y_test), 4))\n",
    "# Check the prediction time for a single sample\n",
    "t=time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26.43 Seconds to train SVC...\n",
      "Test Accuracy of SVC =  0.9959\n"
     ]
    }
   ],
   "source": [
    "svc = LinearSVC(C = clf.best_estimator_.C)\n",
    "# Check the training time for the SVC\n",
    "t=time.time()\n",
    "svc.fit(X_train, y_train)\n",
    "t2 = time.time()\n",
    "print(round(t2-t, 2), 'Seconds to train SVC...')\n",
    "# Check the score of the SVC\n",
    "print('Test Accuracy of SVC = ', round(svc.score(X_test, y_test), 4))\n",
    "# Check the prediction time for a single sample\n",
    "t=time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a function to extract features from a single image window\n",
    "# This function is very similar to extract_features()\n",
    "# just for a single image rather than list of images\n",
    "def single_img_features(img, color_space='RGB', spatial_size=(32, 32),\n",
    "                        hist_bins=32, orient=9, \n",
    "                        pix_per_cell=8, cell_per_block=2, hog_channel=0,\n",
    "                        spatial_feat=True, hist_feat=True, hog_feat=True):    \n",
    "    #1) Define an empty list to receive features\n",
    "    img_features = []\n",
    "    #2) Apply color conversion if other than 'RGB'\n",
    "    if color_space != 'RGB':\n",
    "        if color_space == 'HSV':\n",
    "            feature_image = cv2.cvtColor(img, cv2.COLOR_RGB2HSV)\n",
    "        elif color_space == 'LUV':\n",
    "            feature_image = cv2.cvtColor(img, cv2.COLOR_RGB2LUV)\n",
    "        elif color_space == 'HLS':\n",
    "            feature_image = cv2.cvtColor(img, cv2.COLOR_RGB2HLS)\n",
    "        elif color_space == 'YUV':\n",
    "            feature_image = cv2.cvtColor(img, cv2.COLOR_RGB2YUV)\n",
    "        elif color_space == 'YCrCb':\n",
    "            feature_image = cv2.cvtColor(img, cv2.COLOR_RGB2YCrCb)\n",
    "    else: feature_image = np.copy(img)      \n",
    "    #3) Compute spatial features if flag is set\n",
    "    if spatial_feat == True:\n",
    "        spatial_features = bin_spatial(feature_image, size=spatial_size)\n",
    "        #4) Append features to list\n",
    "        img_features.append(spatial_features)\n",
    "    #5) Compute histogram features if flag is set\n",
    "    if hist_feat == True:\n",
    "        hist_features = color_hist(feature_image, nbins=hist_bins)\n",
    "        #6) Append features to list\n",
    "        img_features.append(hist_features)\n",
    "    #7) Compute HOG features if flag is set\n",
    "    if hog_feat == True:\n",
    "        if hog_channel == 'ALL':\n",
    "            hog_features = []\n",
    "            for channel in range(feature_image.shape[2]):\n",
    "                hog_features.extend(get_hog_features(feature_image[:,:,channel], \n",
    "                                    orient, pix_per_cell, cell_per_block, \n",
    "                                    vis=False, feature_vec=True))      \n",
    "        else:\n",
    "            hog_features = get_hog_features(feature_image[:,:,hog_channel], orient, \n",
    "                        pix_per_cell, cell_per_block, vis=False, feature_vec=True)\n",
    "        #8) Append features to list\n",
    "        img_features.append(hog_features)\n",
    "\n",
    "    #9) Return concatenated array of features\n",
    "    return np.concatenate(img_features)\n",
    "\n",
    "# Define a function you will pass an image \n",
    "# and the list of windows to be searched (output of slide_windows())\n",
    "def search_windows(img, windows, clf, scaler, color_space='RGB', \n",
    "                    spatial_size=(32, 32), hist_bins=32, \n",
    "                    hist_range=(0, 256), orient=9, \n",
    "                    pix_per_cell=8, cell_per_block=2, \n",
    "                    hog_channel=0, spatial_feat=True, \n",
    "                    hist_feat=True, hog_feat=True):\n",
    "\n",
    "    #1) Create an empty list to receive positive detection windows\n",
    "    on_windows = []\n",
    "    #2) Iterate over all windows in the list\n",
    "    for window in windows:\n",
    "        #3) Extract the test window from original image\n",
    "        test_img = cv2.resize(img[window[0][1]:window[1][1], window[0][0]:window[1][0]], (64, 64))      \n",
    "        #4) Extract features for that window using single_img_features()\n",
    "        features = single_img_features(test_img, color_space=color_space, \n",
    "                            spatial_size=spatial_size, hist_bins=hist_bins, \n",
    "                            orient=orient, pix_per_cell=pix_per_cell, \n",
    "                            cell_per_block=cell_per_block, \n",
    "                            hog_channel=hog_channel, spatial_feat=spatial_feat, \n",
    "                            hist_feat=hist_feat, hog_feat=hog_feat)\n",
    "        #5) Scale extracted features to be fed to classifier\n",
    "        test_features = scaler.transform(np.array(features).reshape(1, -1))\n",
    "        #6) Predict using your classifier\n",
    "        prediction = svc.predict(test_features)\n",
    "        \n",
    "        #7) If positive (prediction == 1) then save the window\n",
    "        if prediction == 1:\n",
    "            on_windows.append(window)\n",
    "    #8) Return windows for positive detections\n",
    "    return on_windows\n",
    "\n",
    "# Define a function that takes an image,\n",
    "# start and stop positions in both x and y, \n",
    "# window size (x and y dimensions),  \n",
    "# and overlap fraction (for both x and y)\n",
    "def slide_window(img, x_start_stop=[200, None], y_start_stop=[None, None], \n",
    "                    xy_window=(64, 64), xy_overlap=(0.5, 0.5)):\n",
    "    # If x and/or y start/stop positions not defined, set to image size\n",
    "    if x_start_stop[0] == None:\n",
    "        x_start_stop[0] = 0\n",
    "    if x_start_stop[1] == None:\n",
    "        x_start_stop[1] = img.shape[1]\n",
    "    if y_start_stop[0] == None:\n",
    "        y_start_stop[0] = 0\n",
    "    if y_start_stop[1] == None:\n",
    "        y_start_stop[1] = img.shape[0]\n",
    "    # Compute the span of the region to be searched    \n",
    "    xspan = x_start_stop[1] - x_start_stop[0]\n",
    "    yspan = y_start_stop[1] - y_start_stop[0]\n",
    "    # Compute the number of pixels per step in x/y\n",
    "    nx_pix_per_step = np.int(xy_window[0]*(1 - xy_overlap[0]))\n",
    "    ny_pix_per_step = np.int(xy_window[1]*(1 - xy_overlap[1]))\n",
    "    # Compute the number of windows in x/y\n",
    "    nx_buffer = np.int(xy_window[0]*(xy_overlap[0]))\n",
    "    ny_buffer = np.int(xy_window[1]*(xy_overlap[1]))\n",
    "    nx_windows = np.int((xspan-nx_buffer)/nx_pix_per_step) \n",
    "    ny_windows = np.int((yspan-ny_buffer)/ny_pix_per_step) \n",
    "    # Initialize a list to append window positions to\n",
    "    window_list = []\n",
    "    # Loop through finding x and y window positions\n",
    "    # Note: you could vectorize this step, but in practice\n",
    "    # you'll be considering windows one by one with your\n",
    "    # classifier, so looping makes sense\n",
    "    for ys in range(ny_windows):\n",
    "        for xs in range(nx_windows):\n",
    "            # Calculate window position\n",
    "            startx = xs*nx_pix_per_step + x_start_stop[0]\n",
    "            endx = startx + xy_window[0]\n",
    "            starty = ys*ny_pix_per_step + y_start_stop[0]\n",
    "            endy = starty + xy_window[1]\n",
    "            \n",
    "            # Append window position to list\n",
    "            window_list.append(((startx, starty), (endx, endy)))\n",
    "    # Return the list of windows\n",
    "    return window_list\n",
    "\n",
    "# Define a function to draw bounding boxes\n",
    "def draw_boxes(img, bboxes, color=(0, 0, 255), thick=6):\n",
    "    # Make a copy of the image\n",
    "    imcopy = np.copy(img)\n",
    "    # Iterate through the bounding boxes\n",
    "    for bbox in bboxes:\n",
    "        # Draw a rectangle given bbox coordinates\n",
    "        cv2.rectangle(imcopy, bbox[0], bbox[1], color, thick)\n",
    "    # Return the image copy with boxes drawn\n",
    "    return imcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_heat(heatmap, bbox_list):\n",
    "    # Iterate through list of bboxes\n",
    "    for box in bbox_list:\n",
    "        # Add += 1 for all pixels inside each bbox\n",
    "        # Assuming each \"box\" takes the form ((x1, y1), (x2, y2))\n",
    "        heatmap[box[0][1]:box[1][1], box[0][0]:box[1][0]] += 1\n",
    "\n",
    "    for box in bbox_list:\n",
    "        if(np.max(heatmap[box[0][1]:box[1][1], box[0][0]:box[1][0]])>1):\n",
    "            heatmap[box[0][1]:box[1][1], box[0][0]:box[1][0]] +=2\n",
    "    # Return updated heatmap\n",
    "    return heatmap# Iterate through list of bboxes\n",
    "    \n",
    "def apply_threshold(heatmap, threshold):\n",
    "    # Zero out pixels below the threshold\n",
    "    heatmap[heatmap <= threshold] = 0\n",
    "    # Return thresholded map\n",
    "    return heatmap\n",
    "\n",
    "def draw_labeled_bboxes(img, labels):\n",
    "    # Iterate through all detected cars\n",
    "    for car_number in range(1, labels[1]+1):\n",
    "        # Find pixels with each car_number label value\n",
    "        nonzero = (labels[0] == car_number).nonzero()\n",
    "        # Identify x and y values of those pixels\n",
    "        nonzeroy = np.array(nonzero[0])\n",
    "        nonzerox = np.array(nonzero[1])\n",
    "        # Define a bounding box based on min/max x and y\n",
    "        bbox = ((np.min(nonzerox), np.min(nonzeroy)), (np.max(nonzerox), np.max(nonzeroy)))\n",
    "        # Draw the box on the image\n",
    "        cv2.rectangle(img, bbox[0], bbox[1], (0,0,255), 6)\n",
    "    # Return the image\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "# Read all the  Calibration images\n",
    "image_paths = glob.glob(\"./camera_cal/calibration*.jpg\")\n",
    "\n",
    "## Object Point and Image Point array creation for each image\n",
    "objpoints = [] # Object Points Array \n",
    "imgpoints = [] # Image Points Array \n",
    "pass_img = []\n",
    "\n",
    "#Identify 6*9 object points\n",
    "objp = np.zeros((6*9,3),np.float32)\n",
    "objp[:,:2] = np.mgrid[0:9,0:6].T.reshape(-1,2)\n",
    "\n",
    "# Read each image , find chessboard coreners and draw the chess board corners \n",
    "# Store paths of images in which coreners were found.\n",
    "for img_path in image_paths:\n",
    "    # Read Image from the path\n",
    "    img = cv2.imread(img_path)\n",
    "    # Convert image to gray scale.\n",
    "    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "    # Find Chess board coreners in the image\n",
    "    ret , corners = cv2.findChessboardCorners(gray,(6,9),None)\n",
    "    # If chess board coreners were found store the points in list\n",
    "    if ret == True:\n",
    "        imgpoints.append(corners)\n",
    "        objpoints.append(objp) \n",
    "        img = cv2.drawChessboardCorners(img,(9,6),corners,ret)\n",
    "        pass_img.append(img_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calibrate camera \n",
    "# Accepts image , object points and image points\n",
    "# Return Undistored image\n",
    "def cal_undistort(img, objpoints, imgpoints):\n",
    "    # Convert image to gray scale    \n",
    "    gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)\n",
    "    # Perform Camera Calibration\n",
    "    ret, mtx, dist, rvecs, tvecs = cv2.calibrateCamera(objpoints, imgpoints, gray.shape[::-1], None, None)\n",
    "    # Undistore the images.\n",
    "    undist = cv2.undistort(img, mtx, dist, None, mtx)\n",
    "    return undist\n",
    "\n",
    "# Undistort all the Camera Calibration Images.\n",
    "cnt =0\n",
    "\n",
    "for img_path in pass_img:\n",
    "    # Read in an image\n",
    "    img = cv2.imread(img_path)\n",
    "    # Undistor the image\n",
    "    undistorted = cal_undistort(img, objpoints, imgpoints)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Funtion to perform thresholding \n",
    "# Color image is formed by using combinded binary of s, h and sobelx \n",
    "def threshold(img, s_thresh=(170, 255), sx_thresh=(20, 100)):\n",
    "    img = np.copy(img)\n",
    "    # Convert to HLS color space \n",
    "    hls = cv2.cvtColor(img, cv2.COLOR_RGB2HLS).astype(np.float)\n",
    "    h_channel = hls[:,:,0]\n",
    "    l_channel = hls[:,:,1]\n",
    "    s_channel = hls[:,:,2]\n",
    "    # Convert to Gray Scale\n",
    "    gray_img = cv2.cvtColor(img,cv2.COLOR_RGB2GRAY)\n",
    "    # Sobel x\n",
    "    sobelx = cv2.Sobel(gray_img, cv2.CV_64F, 1, 0) # Take the derivative in x\n",
    "    abs_sobelx = np.absolute(sobelx) # Absolute x derivative to accentuate lines away from horizontal\n",
    "    scaled_sobel = np.uint8(255*abs_sobelx/np.max(abs_sobelx))\n",
    "    \n",
    "    # Threshold x gradient\n",
    "    sxbinary = np.zeros_like(scaled_sobel)\n",
    "    sxbinary[(scaled_sobel >= sx_thresh[0]) & (scaled_sobel <= sx_thresh[1])] = 1\n",
    "    \n",
    "    # S channel Threshold color channel\n",
    "    s_binary = np.zeros_like(s_channel)\n",
    "    s_binary[(s_channel >= s_thresh[0]) & (s_channel <= s_thresh[1])] = 1\n",
    "    \n",
    "    # H channel Threshold color channel\n",
    "    h_binary = np.zeros_like(h_channel)\n",
    "    h_binary[(h_channel >= sx_thresh[0]) & (h_channel <= sx_thresh[1])] = 1\n",
    "    \n",
    "    # Form a combinded image\n",
    "    combinded_binary = np.zeros_like(sxbinary)\n",
    "    combinded_binary[((h_binary==1)&(s_binary==1))|(sxbinary==1)]=1\n",
    "    \n",
    "    # Form a color binary by using combinded binary\n",
    "    color_binary = np.dstack((combinded_binary,combinded_binary,combinded_binary))*255\n",
    "    \n",
    "    return color_binary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define a class to receive the characteristics of each line detection\n",
    "class Line():\n",
    "    def __init__(self):\n",
    "        # was the line detected in the last iteration?\n",
    "        self.detected = False  \n",
    "        # x values of the last n fits of the line\n",
    "        self.recent_xfitted = [] \n",
    "        #average x values of the fitted line over the last n iterations\n",
    "        self.bestx = None     \n",
    "        #polynomial coefficients averaged over the last n iterations\n",
    "        self.best_fit = None  \n",
    "        #polynomial coefficients for the most recent fit\n",
    "        self.current_fit = []  \n",
    "        #radius of curvature of the line in some units\n",
    "        self.radius_of_curvature = None \n",
    "        #distance in meters of vehicle center from the line\n",
    "        self.line_base_pos = None \n",
    "        #difference in fit coefficients between last and new fits\n",
    "        self.diffs = np.array([0,0,0], dtype='float') \n",
    "        #x values for detected line pixels\n",
    "        self.allx = None  \n",
    "        #y values for detected line pixels\n",
    "        self.ally = None\n",
    "        \n",
    "        self.count =0\n",
    "    # Function to update the Lane Lines X , Y and Polynomial Coefficients\n",
    "    def update_coordinates(self,x,y,fit):\n",
    "        # Save X and Y values\n",
    "        self.allx = x\n",
    "        self.ally = y\n",
    "        # Average Filter\n",
    "        self.count +=1\n",
    "        if self.count>20:\n",
    "            self.count = 20\n",
    "            # remove the oldest entry to out_buf if the length of out_buf is more than 'outbuf_len' \n",
    "            self.current_fit.pop(0)\n",
    "            self.recent_xfitted.pop(0)\n",
    "        # Add new values     \n",
    "        self.current_fit.append(fit)\n",
    "        self.recent_xfitted.append(x)\n",
    "        # Average the values\n",
    "        self.bestx = np.average(self.recent_xfitted,axis=0)    \n",
    "        self.best_fit = np.average(self.current_fit,axis=0)    \n",
    "        \n",
    "right_lane_obj = Line()\n",
    "left_lane_obj = Line()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to perform Perspective Transform \n",
    "# Accepts Image to be transformed and Source Points\n",
    "# Returns warped image and Inverse M for reversing the transform in future\n",
    "def perspective_transform(img,src):\n",
    "    # Store the image size in an array\n",
    "    img_size = (img.shape[1],img.shape[0])\n",
    "    # Identify the destination points\n",
    "    dst = np.float32([[(350, 720), (350, 0), (980, 0), (980, 720)]])\n",
    "    # Use CV2 funtion to perform perspective transform \n",
    "    M = cv2.getPerspectiveTransform(src, dst)\n",
    "    # Use CV2 funtion to Inverse Perspective transform for future use\n",
    "    Minv = cv2.getPerspectiveTransform(dst, src)\n",
    "    # Use CV2 funtion and transform the image \n",
    "    warped = cv2.warpPerspective(img, M, img_size, flags=cv2.INTER_LINEAR)\n",
    "    # Return Warped and Inverse Perspective Transform Matrix for future use.\n",
    "    return warped, Minv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to find lanes\n",
    "# Accepts binary warped image\n",
    "# Returns Image with overlayed lanes , polynomial coefficients, x and y values of left, right lanes\n",
    "\n",
    "def find_lanes(binary_warped,left_fit,right_fit):    \n",
    "    # Identify the x and y positions of all nonzero pixels in the image\n",
    "    nonzero = binary_warped.nonzero()\n",
    "    nonzeroy = np.array(nonzero[0])\n",
    "    nonzerox = np.array(nonzero[1])\n",
    "    # Set the width of the windows +/- margin\n",
    "    margin = 100\n",
    "    # Set minimum number of pixels found to recenter window\n",
    "    minpix = 50\n",
    "    # Load the best fit values \n",
    "    left_fit = left_lane_obj.best_fit\n",
    "    right_fit = right_lane_obj.best_fit\n",
    "    # Create an output image to draw on and  visualize the result\n",
    "    out_img = np.dstack((binary_warped, binary_warped, binary_warped))*255\n",
    "    \n",
    "    if((left_fit != None) and (right_fit != None)):\n",
    "        # Assume you now have a new warped binary image \n",
    "        # from the next frame of video (also called \"binary_warped\")\n",
    "        # It's now much easier to find line pixels!\n",
    "        left_lane_inds = ((nonzerox > (left_fit[0]*(nonzeroy**2) + left_fit[1]*nonzeroy + \n",
    "        left_fit[2] - margin)) & (nonzerox < (left_fit[0]*(nonzeroy**2) + \n",
    "        left_fit[1]*nonzeroy + left_fit[2] + margin))) \n",
    "\n",
    "        right_lane_inds = ((nonzerox > (right_fit[0]*(nonzeroy**2) + right_fit[1]*nonzeroy + \n",
    "        right_fit[2] - margin)) & (nonzerox < (right_fit[0]*(nonzeroy**2) + \n",
    "        right_fit[1]*nonzeroy + right_fit[2] + margin)))  \n",
    "\n",
    "        # Again, extract left and right line pixel positions\n",
    "        leftx = nonzerox[left_lane_inds]\n",
    "        lefty = nonzeroy[left_lane_inds] \n",
    "        rightx = nonzerox[right_lane_inds]\n",
    "        righty = nonzeroy[right_lane_inds]\n",
    "         \n",
    "    else:\n",
    "        # Assuming you have created a warped binary image called \"binary_warped\"\n",
    "        # Take a histogram of the bottom half of the image\n",
    "        histogram = np.sum(binary_warped[np.int32(binary_warped.shape[0]/2):,:], axis=0)\n",
    "        \n",
    "        # Find the peak of the left and right halves of the histogram\n",
    "        # These will be the starting point for the left and right lines\n",
    "        midpoint = np.int(histogram.shape[0]/2)\n",
    "        leftx_base = np.argmax(histogram[:midpoint])\n",
    "        rightx_base = np.argmax(histogram[midpoint:]) + midpoint\n",
    "\n",
    "        # Choose the number of sliding windows\n",
    "        nwindows = 9\n",
    "        # Set height of windows\n",
    "        window_height = np.int(binary_warped.shape[0]/nwindows)\n",
    "        \n",
    "        # Current positions to be updated for each window\n",
    "        leftx_current = leftx_base\n",
    "        rightx_current = rightx_base\n",
    "        \n",
    "        # Create empty lists to receive left and right lane pixel indices\n",
    "        left_lane_inds = []\n",
    "        right_lane_inds = []\n",
    "\n",
    "        # Step through the windows one by one\n",
    "        for window in range(nwindows):\n",
    "            # Identify window boundaries in x and y (and right and left)\n",
    "            win_y_low = binary_warped.shape[0] - (window+1)*window_height\n",
    "            win_y_high = binary_warped.shape[0] - window*window_height\n",
    "            win_xleft_low = leftx_current - margin\n",
    "            win_xleft_high = leftx_current + margin\n",
    "            win_xright_low = rightx_current - margin\n",
    "            win_xright_high = rightx_current + margin\n",
    "            # Draw the windows on the visualization image\n",
    "            cv2.rectangle(out_img,(win_xleft_low,win_y_low),(win_xleft_high,win_y_high),\n",
    "            (0,255,0), 2) \n",
    "            cv2.rectangle(out_img,(win_xright_low,win_y_low),(win_xright_high,win_y_high),\n",
    "            (0,255,0), 2) \n",
    "            # Identify the nonzero pixels in x and y within the window\n",
    "            good_left_inds = ((nonzeroy >= win_y_low) & (nonzeroy < win_y_high) & \n",
    "            (nonzerox >= win_xleft_low) &  (nonzerox < win_xleft_high)).nonzero()[0]\n",
    "            good_right_inds = ((nonzeroy >= win_y_low) & (nonzeroy < win_y_high) & \n",
    "            (nonzerox >= win_xright_low) &  (nonzerox < win_xright_high)).nonzero()[0]\n",
    "            # Append these indices to the lists\n",
    "            left_lane_inds.append(good_left_inds)\n",
    "            right_lane_inds.append(good_right_inds)\n",
    "            # If you found > minpix pixels, recenter next window on their mean position\n",
    "            if len(good_left_inds) > minpix:\n",
    "                leftx_current = np.int(np.mean(nonzerox[good_left_inds]))\n",
    "            if len(good_right_inds) > minpix:        \n",
    "                rightx_current = np.int(np.mean(nonzerox[good_right_inds]))\n",
    "\n",
    "        # Concatenate the arrays of indices\n",
    "        left_lane_inds = np.concatenate(left_lane_inds)\n",
    "        right_lane_inds = np.concatenate(right_lane_inds)\n",
    "\n",
    "        # Extract left and right line pixel positions\n",
    "        leftx = nonzerox[left_lane_inds]\n",
    "        lefty = nonzeroy[left_lane_inds] \n",
    "        rightx = nonzerox[right_lane_inds]\n",
    "        righty = nonzeroy[right_lane_inds] \n",
    "        \n",
    "        # Highlight Left Lane in Red and Right in Blue    \n",
    "        out_img[nonzeroy[left_lane_inds], nonzerox[left_lane_inds]] = [255, 0, 0]\n",
    "        out_img[nonzeroy[right_lane_inds], nonzerox[right_lane_inds]] = [0, 0, 255]\n",
    "            \n",
    "\n",
    "    # Fit a second order polynomial to each\n",
    "    left_fit = np.polyfit(lefty, leftx, 2)\n",
    "    right_fit = np.polyfit(righty, rightx, 2)\n",
    "    \n",
    "    # Create Evenly Spaced Y values and extract x values for each x \n",
    "    ploty = np.linspace(0, binary_warped.shape[0]-1, binary_warped.shape[0] )\n",
    "    left_fitx = left_fit[0]*ploty**2 + left_fit[1]*ploty + left_fit[2]\n",
    "    right_fitx = right_fit[0]*ploty**2 + right_fit[1]*ploty + right_fit[2]\n",
    "    \n",
    "    # Limit the X to be with in the image boundaries\n",
    "    left_fitx[(left_fitx>1280)]=1279\n",
    "    right_fitx[(right_fitx>1280)]=1279\n",
    "    left_fitx[left_fitx<0]=0\n",
    "    right_fitx[right_fitx<0]=0\n",
    "    \n",
    "    # Highlight the Left and Right Lanes in Green \n",
    "    out_img[np.int32(ploty),np.int32(left_fitx)] = [255,255,0]\n",
    "    out_img[np.int32(ploty),np.int32(right_fitx)] = [255,255,0]\n",
    "    \n",
    "    # Return Image with overlayed lanes , polynomial coefficients, x and y values of left, right lanes \n",
    "    return out_img,np.dstack((left_fit,right_fit)),np.dstack((ploty,left_fitx,right_fitx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "image = mpimg.imread(\"./test_images/test5.jpg\")\n",
    "# Function to draw lanes and fill the space between lanes in green\n",
    "# Input: Warped Image , Original Undistored image , Left & Right X , Inverse Perspective Matrix\n",
    "# Returns: Image with overlayed lanes and area between lanes filled in green\n",
    "def draw_lines(warped,undist,ploty,left_fitx,right_fitx,Minv):\n",
    "    margin = 15\n",
    "    # Create an image to draw the lines on\n",
    "    warp_zero = np.zeros_like(warped).astype(np.uint8)\n",
    "    color_warp = np.dstack((warp_zero, warp_zero, warp_zero))\n",
    "    \n",
    "    # Recast the x and y points into usable format for cv2.fillPoly()\n",
    "    pts_left = np.array([np.transpose(np.vstack([left_fitx, ploty]))])\n",
    "    pts_right = np.array([np.flipud(np.transpose(np.vstack([right_fitx, ploty])))])\n",
    "    pts = np.hstack((pts_left, pts_right))\n",
    "\n",
    "    # Draw the lane onto the warped blank image\n",
    "    cv2.fillPoly(color_warp, np.int_([pts]), (0,255, 0))\n",
    "    \n",
    "    # Draw the lane onto the warped blank image\n",
    "    # Generate a polygon to illustrate the search window area\n",
    "    # And recast the x and y points into usable format for cv2.fillPoly()\n",
    "    left_line_window1 = np.array([np.transpose(np.vstack([left_fitx-margin, ploty]))])\n",
    "    left_line_window2 = np.array([np.flipud(np.transpose(np.vstack([left_fitx+margin, \n",
    "                              ploty])))])\n",
    "    left_line_pts = np.hstack((left_line_window1, left_line_window2))\n",
    "    right_line_window1 = np.array([np.transpose(np.vstack([right_fitx-margin, ploty]))])\n",
    "    right_line_window2 = np.array([np.flipud(np.transpose(np.vstack([right_fitx+margin, \n",
    "                              ploty])))])\n",
    "    right_line_pts = np.hstack((right_line_window1, right_line_window2))\n",
    "    \n",
    "    # Highlight Left Lane in Red and Right in Blue \n",
    "    cv2.fillPoly(color_warp, np.int_([left_line_pts]), (255,0, 0))\n",
    "    cv2.fillPoly(color_warp, np.int_([right_line_pts]), (0,0,255))\n",
    "    \n",
    "    # Warp the blank back to original image space using inverse perspective matrix (Minv)\n",
    "    newwarp = cv2.warpPerspective(color_warp, Minv, (image.shape[1], image.shape[0])) \n",
    "    # Combine the result with the original image\n",
    "    result = cv2.addWeighted(undist, 1, newwarp, 0.3, 0)\n",
    "    \n",
    "    return(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Function to calculate Radius of Curvature at the vehicle edge\n",
    "# Input: Y Points , Left and Right Lane X values\n",
    "# Output: Left and Right Lange Curvature\n",
    "def cal_radius_curvature(ploty,leftx,rightx):\n",
    "    # Define conversions in x and y from pixels space to meters\n",
    "    ym_per_pix = 30/720 # meters per pixel in y dimension\n",
    "    xm_per_pix = 3.7/700 # meters per pixel in x dimension\n",
    "    \n",
    "    # Evaluate the radius at the edge of the vehicle \n",
    "    y_eval = np.max(ploty)\n",
    "    \n",
    "    # Fit new polynomials to x,y in world space\n",
    "    left_fit_cr = np.polyfit(ploty*ym_per_pix, leftx*xm_per_pix, 2)\n",
    "    right_fit_cr = np.polyfit(ploty*ym_per_pix, rightx*xm_per_pix, 2)\n",
    "    \n",
    "    # Calculate the new radii of curvature\n",
    "    left_curverad = ((1 + (2*left_fit_cr[0]*y_eval*ym_per_pix + left_fit_cr[1])**2)**1.5) / np.absolute(2*left_fit_cr[0])\n",
    "    right_curverad = ((1 + (2*right_fit_cr[0]*y_eval*ym_per_pix + right_fit_cr[1])**2)**1.5) / np.absolute(2*right_fit_cr[0])\n",
    "    \n",
    "    # Now our radius of curvature is in meters\n",
    "    return left_curverad,right_curverad    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Source Points for Perspecitive Transformation\n",
    "points = np.float32([[(200, 720), (570, 470), (720, 470), (1130, 720)]])\n",
    "# window settings\n",
    "window_width = 50 \n",
    "window_height = 80 # Break image into 9 vertical layers since image height is 720\n",
    "margin = 100 # How much to slide left and right for searching\n",
    "# Pipeline to process the images\n",
    "# Input: Raw Image from Video\n",
    "# Return: Processed Image with overlayed Lane Lines\n",
    "def process_image_lane(image):\n",
    "    # Clear Pervious Image Curve values and Radius of curvature.\n",
    "    left_fit =[]\n",
    "    right_fit =[]\n",
    "    radius = []\n",
    "    rad_count =0\n",
    "    \n",
    "    #### PipeLine Start #####\n",
    "    # Undistort the Image\n",
    "    undistorted = cal_undistort(image, objpoints, imgpoints)\n",
    "    # Perform Thresholding\n",
    "    image_gray = threshold(undistorted)\n",
    "    # Perspective Transform the image\n",
    "    warped, persp_invM = perspective_transform(image_gray,points)\n",
    "    # Convert to Gray scale \n",
    "    warped_gray = cv2.cvtColor(warped,cv2.COLOR_BGR2GRAY)\n",
    "    # Find Lane Lines \n",
    "    out_img,poly_fit,coord_fit = find_lanes(warped_gray,left_fit,right_fit)\n",
    "    # Store Left and Right Polynomial\n",
    "    left_fit = poly_fit[0,:,0]\n",
    "    right_fit = poly_fit[0,:,1]\n",
    "    # Update the Left and Right Object Parameters\n",
    "    left_lane_obj.update_coordinates(coord_fit[0,:,1],coord_fit[0,:,0],(left_fit))\n",
    "    right_lane_obj.update_coordinates(coord_fit[0,:,2],coord_fit[0,:,0],(right_fit))\n",
    "    # Draw the Lane Lines on the image\n",
    "    out_img = draw_lines(warped_gray,undistorted,coord_fit[0,:,0],coord_fit[0,:,1],coord_fit[0,:,2],\n",
    "                         persp_invM)\n",
    "    \n",
    "    #######  Calculate the Radius of curvature #####\n",
    "    left_lane_obj.radius_of_curvature,right_lane_obj.radius_of_curvature = cal_radius_curvature(coord_fit[0,:,0],left_lane_obj.bestx,right_lane_obj.bestx)\n",
    "    # Average the Radius of curvature over 60 samples\n",
    "    radius.append((left_lane_obj.radius_of_curvature+right_lane_obj.radius_of_curvature)/2)\n",
    "    rad_count+=1\n",
    "    if(rad_count>60):\n",
    "        rad_count=60\n",
    "        radius.pop(0)\n",
    "    # Print the radius of curvature    \n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    cv2.putText(out_img,'Radius of Curvature = '+str(np.average(radius))+'(m)',(100,30), font, 1, (255,255,255), 2, cv2.LINE_AA)\n",
    "    \n",
    "    # Calculte the offest\n",
    "    offset = ((coord_fit[0,:,1][(image.shape[0]-1)]+coord_fit[0,:,2][(image.shape[0]-1)]-image.shape[1])*(3.7/1400))\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    # Print the offset\n",
    "    cv2.putText(out_img,'Vehicle is '+str(offset)+'(m) left of center',(100,80), font, 1, (255,255,255), 2, cv2.LINE_AA)\n",
    "    \n",
    "    # Return the overlayed image\n",
    "    return out_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_start_stop = [400, 660] # Min and max in y to search in slide_window()\n",
    "heatmaps_queue = collections.deque(maxlen=15)\n",
    "\n",
    "def process_video(image):\n",
    "    # This function is simlar to process_image and implements the pipeline for video processing\n",
    "    # Given image is searched 2 times with scaling 96x96 and 128x128 respectively\n",
    "    # Two searches are done sequentially.\n",
    "    # After every search a heatmap is developed by highlighting windows with cars in red rest all image is blackend\n",
    "    # In the areas where boxes overlap intensity of the red pixels is increased to increase the final box size.\n",
    "    # To reduce the wobling a queue is formed out of last 15 frames and added , thresholding is applied over the sum.\n",
    "    # Boundaries of the red blob are identified and boxes are drawn on the final image.\n",
    "    # This function generates 1 outputs draw_img is final output\n",
    "    \n",
    "    global heatmaps_queue\n",
    "    \n",
    "    draw_image = np.copy(cv2.cvtColor(image,cv2.COLOR_BGR2RGB))\n",
    "    heat = np.zeros_like(image[:,:,0]).astype(np.float)\n",
    "    out_img = process_image_lane(image)\n",
    "    windows = slide_window(image, x_start_stop=[400, 1100], y_start_stop= [375,600], \n",
    "                    xy_window=(96, 96), xy_overlap=(0.75, 0.75))\n",
    "    \n",
    "    hot_windows = search_windows(image, windows, svc, X_scaler, color_space=color_space, \n",
    "                        spatial_size=spatial_size, hist_bins=hist_bins, \n",
    "                        orient=orient, pix_per_cell=pix_per_cell, \n",
    "                        cell_per_block=cell_per_block, \n",
    "                        hog_channel=hog_channel, spatial_feat=spatial_feat, \n",
    "                        hist_feat=hist_feat, hog_feat=hog_feat) \n",
    "    # Add heat to each box in box list\n",
    "    heat = add_heat(heat,hot_windows)\n",
    "    \n",
    "    windows = slide_window(image, x_start_stop=[400, None], y_start_stop=y_start_stop, \n",
    "                    xy_window=(128, 128), xy_overlap=(0.75, 0.75))\n",
    "    \n",
    "    hot_windows = search_windows(image, windows, svc, X_scaler, color_space=color_space, \n",
    "                        spatial_size=spatial_size, hist_bins=hist_bins, \n",
    "                        orient=orient, pix_per_cell=pix_per_cell, \n",
    "                        cell_per_block=cell_per_block, \n",
    "                        hog_channel=hog_channel, spatial_feat=spatial_feat, \n",
    "                        hist_feat=hist_feat, hog_feat=hog_feat) \n",
    "    # Add heat to each box in box list\n",
    "    heat = add_heat(heat,hot_windows)\n",
    "    \n",
    "    heatmaps_queue.append(heat)\n",
    "    heatmap_sum = sum(heatmaps_queue)\n",
    "    \n",
    "    # Apply threshold to help remove false positives\n",
    "    heat = apply_threshold(heatmap_sum,10)\n",
    "    #heat = apply_threshold(heat,2)\n",
    "\n",
    "    # Visualize the heatmap when displaying    \n",
    "    heatmap = np.clip(heat, 0, 255)\n",
    "\n",
    "    # Find final boxes from heatmap using label function\n",
    "    labels = label(heatmap)\n",
    "    draw_img = draw_labeled_bboxes(np.copy(out_img), labels)\n",
    "    \n",
    "    return draw_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MoviePy] >>>> Building video output_videos/challenge_video.mp4\n",
      "[MoviePy] Writing audio in challenge_videoTEMP_MPY_wvf_snd.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 1112/1112 [00:00<00:00, 2497.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MoviePy] Done.\n",
      "[MoviePy] Writing video output_videos/challenge_video.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████▉| 1260/1261 [39:24<00:01,  2.00s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MoviePy] Done.\n",
      "[MoviePy] >>>> Video ready: output_videos/challenge_video.mp4 \n",
      "\n",
      "Wall time: 39min 25s\n"
     ]
    }
   ],
   "source": [
    "from moviepy.editor import VideoFileClip\n",
    "from IPython.display import HTML\n",
    "from moviepy.editor import * \n",
    "\n",
    "heatmaps_queue.clear()\n",
    "cap = VideoFileClip('project_video.mp4')\n",
    "modified_clip = cap.fl_image( process_video )\n",
    "%time modified_clip.write_videofile(\"output_videos/challenge_video.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video width=\"960\" height=\"540\" controls>\n",
       "  <source src=\"output_videos/challenge_video.mp4\">\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "HTML(\"\"\"\n",
    "<video width=\"960\" height=\"540\" controls>\n",
    "  <source src=\"{0}\">\n",
    "</video>\n",
    "\"\"\".format(\"output_videos/challenge_video.mp4\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
